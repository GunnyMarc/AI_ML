{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Level 101: A Beginner's Guide to GPU Programming\n",
    "\n",
    "Welcome to this comprehensive introduction to CUDA programming!\n",
    "\n",
    "## What You'll Learn\n",
    "1. What is CUDA and why use it?\n",
    "2. Understanding GPU architecture\n",
    "3. Setting up your environment\n",
    "4. Your first CUDA program\n",
    "5. Memory management\n",
    "6. Working with real data (cuda_sample_data.csv)\n",
    "7. Performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is CUDA?\n",
    "\n",
    "**CUDA** (Compute Unified Device Architecture) is NVIDIA's parallel computing platform.\n",
    "\n",
    "### Why Use CUDA?\n",
    "- **Massive Parallelism**: Thousands of cores executing simultaneously\n",
    "- **Performance**: Orders of magnitude faster for parallelizable tasks\n",
    "- **Applications**: ML, scientific computing, image processing\n",
    "\n",
    "| Feature | CPU | GPU |\n",
    "|---------|-----|-----|\n",
    "| Cores | Few (4-64) | Many (thousands) |\n",
    "| Best For | Sequential tasks | Parallel tasks |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up Your Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NVIDIA driver and CUDA\n",
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numba cupy-cuda12x numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "\n",
    "print(f\"CUDA available: {cuda.is_available()}\")\n",
    "if cuda.is_available():\n",
    "    print(f\"CUDA Device: {cuda.get_current_device().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding GPU Architecture\n",
    "\n",
    "### Key Concepts\n",
    "- **Host**: CPU and its memory\n",
    "- **Device**: GPU and its memory\n",
    "- **Kernel**: Function running on GPU\n",
    "- **Thread**: Smallest execution unit\n",
    "- **Block**: Group of cooperating threads\n",
    "- **Grid**: Collection of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPU information\n",
    "if cuda.is_available():\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"Device: {device.name}\")\n",
    "    print(f\"Compute Capability: {device.compute_capability}\")\n",
    "    print(f\"Max Threads/Block: {device.MAX_THREADS_PER_BLOCK}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Your First CUDA Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple vector addition kernel\n",
    "@cuda.jit\n",
    "def add_kernel(a, b, result):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < a.size:\n",
    "        result[idx] = a[idx] + b[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the kernel\n",
    "n = 1000000\n",
    "a = np.ones(n, dtype=np.float32)\n",
    "b = np.ones(n, dtype=np.float32) * 2\n",
    "result = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "# Copy to device\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_result = cuda.to_device(result)\n",
    "\n",
    "# Launch kernel\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "add_kernel[blocks, threads](d_a, d_b, d_result)\n",
    "\n",
    "# Get result\n",
    "result = d_result.copy_to_host()\n",
    "print(f\"Result: {result[:10]}\")\n",
    "print(f\"All correct: {np.allclose(result, 3.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory Management\n",
    "\n",
    "### Memory Types\n",
    "1. **Global Memory**: Largest, slowest\n",
    "2. **Shared Memory**: Fast, shared within block\n",
    "3. **Registers**: Fastest, very limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication with shared memory\n",
    "from numba import float32\n",
    "\n",
    "TILE_SIZE = 16\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_shared(A, B, C):\n",
    "    sA = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=float32)\n",
    "    sB = cuda.shared.array((TILE_SIZE, TILE_SIZE), dtype=float32)\n",
    "    \n",
    "    x, y = cuda.grid(2)\n",
    "    tx, ty = cuda.threadIdx.x, cuda.threadIdx.y\n",
    "    \n",
    "    if x >= C.shape[0] or y >= C.shape[1]:\n",
    "        return\n",
    "    \n",
    "    tmp = 0.0\n",
    "    for i in range((A.shape[1] + TILE_SIZE - 1) // TILE_SIZE):\n",
    "        if x < A.shape[0] and (i * TILE_SIZE + ty) < A.shape[1]:\n",
    "            sA[tx, ty] = A[x, i * TILE_SIZE + ty]\n",
    "        else:\n",
    "            sA[tx, ty] = 0.0\n",
    "        if y < B.shape[1] and (i * TILE_SIZE + tx) < B.shape[0]:\n",
    "            sB[tx, ty] = B[i * TILE_SIZE + tx, y]\n",
    "        else:\n",
    "            sB[tx, ty] = 0.0\n",
    "        cuda.syncthreads()\n",
    "        for j in range(TILE_SIZE):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "        cuda.syncthreads()\n",
    "    C[x, y] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Working with Real Data: cuda_sample_data.csv\n",
    "\n",
    "Let's process our 100,000 record dataset with GPU acceleration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('cuda_sample_data.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vector data\n",
    "vec_a_x = df['vector_a_x'].values.astype(np.float32)\n",
    "vec_a_y = df['vector_a_y'].values.astype(np.float32)\n",
    "vec_a_z = df['vector_a_z'].values.astype(np.float32)\n",
    "vec_b_x = df['vector_b_x'].values.astype(np.float32)\n",
    "vec_b_y = df['vector_b_y'].values.astype(np.float32)\n",
    "vec_b_z = df['vector_b_z'].values.astype(np.float32)\n",
    "\n",
    "print(f\"Loaded {len(vec_a_x):,} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA kernels for vector operations\n",
    "@cuda.jit\n",
    "def dot_product_kernel(ax, ay, az, bx, by, bz, result):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < ax.size:\n",
    "        result[idx] = ax[idx]*bx[idx] + ay[idx]*by[idx] + az[idx]*bz[idx]\n",
    "\n",
    "@cuda.jit\n",
    "def magnitude_kernel(x, y, z, result):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < x.size:\n",
    "        result[idx] = math.sqrt(x[idx]**2 + y[idx]**2 + z[idx]**2)\n",
    "\n",
    "@cuda.jit\n",
    "def cross_product_kernel(ax, ay, az, bx, by, bz, rx, ry, rz):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < ax.size:\n",
    "        rx[idx] = ay[idx]*bz[idx] - az[idx]*by[idx]\n",
    "        ry[idx] = az[idx]*bx[idx] - ax[idx]*bz[idx]\n",
    "        rz[idx] = ax[idx]*by[idx] - ay[idx]*bx[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process on GPU\n",
    "n = len(vec_a_x)\n",
    "\n",
    "# Transfer to device\n",
    "d_ax, d_ay, d_az = cuda.to_device(vec_a_x), cuda.to_device(vec_a_y), cuda.to_device(vec_a_z)\n",
    "d_bx, d_by, d_bz = cuda.to_device(vec_b_x), cuda.to_device(vec_b_y), cuda.to_device(vec_b_z)\n",
    "\n",
    "# Allocate results\n",
    "d_dots = cuda.device_array(n, dtype=np.float32)\n",
    "d_mags = cuda.device_array(n, dtype=np.float32)\n",
    "\n",
    "# Configure and run\n",
    "threads, blocks = 256, (n + 255) // 256\n",
    "\n",
    "start = time.time()\n",
    "dot_product_kernel[blocks, threads](d_ax, d_ay, d_az, d_bx, d_by, d_bz, d_dots)\n",
    "magnitude_kernel[blocks, threads](d_ax, d_ay, d_az, d_mags)\n",
    "cuda.synchronize()\n",
    "gpu_time = time.time() - start\n",
    "\n",
    "# Get results\n",
    "dots = d_dots.copy_to_host()\n",
    "mags = d_mags.copy_to_host()\n",
    "\n",
    "print(f\"GPU Time: {gpu_time:.4f}s\")\n",
    "print(f\"Dot products: {dots[:5]}\")\n",
    "print(f\"Magnitudes: {mags[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CPU vs GPU Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU implementation\n",
    "start = time.time()\n",
    "cpu_dots = vec_a_x*vec_b_x + vec_a_y*vec_b_y + vec_a_z*vec_b_z\n",
    "cpu_mags = np.sqrt(vec_a_x**2 + vec_a_y**2 + vec_a_z**2)\n",
    "cpu_time = time.time() - start\n",
    "\n",
    "print(f\"CPU Time: {cpu_time:.4f}s\")\n",
    "print(f\"GPU Time: {gpu_time:.4f}s\")\n",
    "print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "print(f\"Results match: {np.allclose(dots, cpu_dots) and np.allclose(mags, cpu_mags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### What We Covered\n",
    "- CUDA fundamentals and GPU architecture\n",
    "- Writing and launching kernels\n",
    "- Memory management\n",
    "- Processing real data with GPU acceleration\n",
    "\n",
    "### Resources\n",
    "- [NVIDIA CUDA Documentation](https://docs.nvidia.com/cuda/)\n",
    "- [Numba CUDA Documentation](https://numba.pydata.org/numba-doc/latest/cuda/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Implement vector normalization on GPU\n",
    "@cuda.jit\n",
    "def normalize_kernel(x, y, z, rx, ry, rz):\n",
    "    # Your code here\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
