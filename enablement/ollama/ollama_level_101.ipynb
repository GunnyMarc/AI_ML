{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Level 101: A Beginner's Complete Guide\n",
    "\n",
    "Welcome to this comprehensive introduction to Ollama!\n",
    "\n",
    "## What You'll Learn\n",
    "1. What is Ollama?\n",
    "2. Installing and setting up\n",
    "3. Using the API\n",
    "4. Python integration\n",
    "5. Working with real data (ollama_sample_data.csv)\n",
    "6. Building a Q&A system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Ollama?\n",
    "\n",
    "**Ollama** runs LLMs locally on your machine.\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| Privacy | Data stays local |\n",
    "| Cost | Free after setup |\n",
    "| Offline | Works without internet |\n",
    "| Customizable | Create specialized models |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Ollama installation\n",
    "!which ollama || echo \"Install from: https://ollama.com/download\"\n",
    "!ollama --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python packages\n",
    "!pip install ollama pandas numpy requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Check available models\n",
    "try:\n",
    "    models = ollama.list()\n",
    "    print(\"Available models:\")\n",
    "    for m in models.get('models', []):\n",
    "        print(f\"  - {m['name']}\")\n",
    "except:\n",
    "    print(\"Start Ollama server: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple generation\n",
    "def generate(model, prompt):\n",
    "    try:\n",
    "        response = ollama.generate(model=model, prompt=prompt)\n",
    "        return response['response']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example (uncomment if you have a model)\n",
    "# print(generate('llama3.2:1b', 'What is Python in one sentence?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat conversation\n",
    "def chat(model, messages):\n",
    "    try:\n",
    "        response = ollama.chat(model=model, messages=messages)\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are helpful.'},\n",
    "    {'role': 'user', 'content': 'Hello!'}\n",
    "]\n",
    "# print(chat('llama3.2:1b', messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_chat(model, prompt):\n",
    "    try:\n",
    "        stream = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            stream=True\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            print(chunk['message']['content'], end='', flush=True)\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Example\n",
    "# stream_chat('llama3.2:1b', 'Tell me a short joke.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Modelfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Modelfile\n",
    "modelfile = \"\"\"FROM llama3.2:1b\n",
    "SYSTEM You are a Python expert. Be concise.\n",
    "PARAMETER temperature 0.7\n",
    "\"\"\"\n",
    "\n",
    "# Save it\n",
    "with open('Modelfile.python', 'w') as f:\n",
    "    f.write(modelfile)\n",
    "\n",
    "print(\"Modelfile created!\")\n",
    "print(\"Create model: ollama create python-expert -f Modelfile.python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Working with Real Data: ollama_sample_data.csv\n",
    "\n",
    "Let's process our 100,000 record text dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('ollama_sample_data.csv')\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nTopic distribution:\\n{df['topic'].value_counts()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample questions by topic\n",
    "print(\"Sample Questions:\\n\")\n",
    "for topic in df['topic'].unique()[:4]:\n",
    "    sample = df[df['topic'] == topic].iloc[0]\n",
    "    print(f\"[{topic.upper()}] {sample['question']}\")\n",
    "    print(f\"Context: {sample['context'][:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question answering with context\n",
    "def answer_with_context(model, question, context):\n",
    "    prompt = f\"\"\"Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer concisely:\"\"\"\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options={'num_predict': 100, 'temperature': 0.7}\n",
    "        )\n",
    "        return response['response']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example (uncomment to run)\n",
    "# sample = df.iloc[0]\n",
    "# answer = answer_with_context('llama3.2:1b', sample['question'], sample['context'])\n",
    "# print(f\"Q: {sample['question']}\")\n",
    "# print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch question classification\n",
    "def classify_complexity(model, question):\n",
    "    prompt = f\"\"\"Classify as 'simple', 'intermediate', or 'advanced'. One word only.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Complexity:\"\"\"\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model, prompt=prompt,\n",
    "            options={'num_predict': 5, 'temperature': 0.1}\n",
    "        )\n",
    "        return response['response'].strip().lower()\n",
    "    except:\n",
    "        return 'error'\n",
    "\n",
    "# Example batch processing\n",
    "# for i, row in df.head(3).iterrows():\n",
    "#     pred = classify_complexity('llama3.2:1b', row['question'])\n",
    "#     print(f\"Actual: {row['complexity']}, Pred: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building a Q&A System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleQA:\n",
    "    def __init__(self, df, model='llama3.2:1b'):\n",
    "        self.df = df\n",
    "        self.model = model\n",
    "    \n",
    "    def search(self, query, topic=None, n=5):\n",
    "        \"\"\"Find relevant questions using keyword matching.\"\"\"\n",
    "        subset = self.df if topic is None else self.df[self.df['topic'] == topic]\n",
    "        query_words = set(query.lower().split())\n",
    "        \n",
    "        scores = []\n",
    "        for idx, row in subset.iterrows():\n",
    "            q_words = set(row['question'].lower().split())\n",
    "            overlap = len(query_words & q_words)\n",
    "            scores.append((idx, overlap, row))\n",
    "        \n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores[:n]\n",
    "    \n",
    "    def answer(self, query, topic=None):\n",
    "        \"\"\"Find and answer using context.\"\"\"\n",
    "        results = self.search(query, topic)\n",
    "        if not results:\n",
    "            return None\n",
    "        \n",
    "        best = results[0][2]\n",
    "        return {\n",
    "            'question': best['question'],\n",
    "            'topic': best['topic'],\n",
    "            'context': best['context'],\n",
    "            'complexity': best['complexity']\n",
    "        }\n",
    "\n",
    "# Create Q&A system\n",
    "qa = SimpleQA(df)\n",
    "\n",
    "# Test search\n",
    "result = qa.answer('machine learning')\n",
    "print(f\"Found: {result['question']}\")\n",
    "print(f\"Topic: {result['topic']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chatbot\n",
    "class ChatBot:\n",
    "    def __init__(self, model='llama3.2:1b', system=None):\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        if system:\n",
    "            self.messages.append({'role': 'system', 'content': system})\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        self.messages.append({'role': 'user', 'content': user_input})\n",
    "        try:\n",
    "            response = ollama.chat(model=self.model, messages=self.messages)\n",
    "            reply = response['message']['content']\n",
    "            self.messages.append({'role': 'assistant', 'content': reply})\n",
    "            return reply\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    def reset(self):\n",
    "        system = self.messages[0] if self.messages and self.messages[0]['role'] == 'system' else None\n",
    "        self.messages = [system] if system else []\n",
    "\n",
    "# Example\n",
    "# bot = ChatBot('llama3.2:1b', 'You are helpful and concise.')\n",
    "# print(bot.chat('Hello!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Embeddings (RAG Basics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_embedding(text, model='nomic-embed-text'):\n",
    "    try:\n",
    "        response = ollama.embeddings(model=model, prompt=text)\n",
    "        return response['embedding']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Example semantic search\n",
    "# emb1 = get_embedding('machine learning')\n",
    "# emb2 = get_embedding('artificial intelligence')\n",
    "# print(f\"Similarity: {cosine_sim(emb1, emb2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Covered\n",
    "- Installing and configuring Ollama\n",
    "- Generation and chat APIs\n",
    "- Custom Modelfiles\n",
    "- Processing text data (100,000 records)\n",
    "- Building Q&A systems\n",
    "- Embeddings for semantic search\n",
    "\n",
    "### Useful Commands\n",
    "```bash\n",
    "ollama list          # List models\n",
    "ollama pull <model>  # Download\n",
    "ollama run <model>   # Interactive chat\n",
    "ollama serve         # Start API server\n",
    "```\n",
    "\n",
    "### Resources\n",
    "- [Ollama Website](https://ollama.com/)\n",
    "- [Ollama Library](https://ollama.com/library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import os\n",
    "if os.path.exists('Modelfile.python'):\n",
    "    os.remove('Modelfile.python')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
